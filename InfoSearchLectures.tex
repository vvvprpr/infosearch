\documentclass[bachelor, och, referat, times]{resourses/SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage{graphicx}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{verbatim}
\usepackage{longtable}
\usepackage{array}
\usepackage[english,russian]{babel}
\usepackage{tikz}
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{multirow}
\usepackage{enumitem}


\usepackage[colorlinks=false]{hyperref}

\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Введение в информационный поиск}

% Курс
\course{4}

% Группа
\group{451}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
%\department{факультета КНиИТ}

% Специальность/направление код - наименование
%\napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
%\napravlenie{010500 "--- Математическое обеспечение и администрирование информационных систем}
%\napravlenie{230100 "--- Информатика и вычислительная техника}
\napravlenie{09.03.04 "--- Программная инженерия}
%\napravlenie{090301 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
\studenttitle{Студентов}

% Фамилия, имя, отчество в родительном падеже
%\author{451 группы}

% Заведующий кафедрой
\chtitle{к.\,ф.-м.\,н.} % степень, звание
\chname{С.\,В.\,Миронов} %Миронов\;С.\,В.

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент, к.\,ф.-м.\,н.} %должность, степень, звание
\saname{А.\,А.\,Кузнецов}

% Руководитель практики от организации (только для практики,
% для остальных типов работ не используется)
\patitle{руководитель ЦОПП"=СГУ}
\paname{М.~Р.~Мирзаянов}

% Семестр (только для практики, для остальных
% типов работ не используется)
\term{2}

% Наименование практики (только для практики, для остальных
% типов работ не используется)
\practtype{учебная} %Учебная или производственная

% Продолжительность практики (количество недель) (только для практики,
% для остальных типов работ не используется)
\duration{2}

% Даты начала и окончания практики (только для практики, для остальных
% типов работ не используется)
\practStart{01.07.2016}
\practFinish{14.07.2016}

% Год выполнения отчета
\date{2018}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам 1.1, 1.2, 2.1, ...
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
%\abbreviations

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr

% Раздел "Введение"
\intro

Задание 1. Исправление опечаток. Есть словарь с правильными словами и есть документ с опечатками. Взять готовый словарь на 600 тысяч -- миллион слов. Попробовать и сравнить два варианта алгоритма -- простой и с k"=грамм индексом.

Задание 2. Выбрать сайт, в котором есть какой"=нибудь каталог (книжек, фильмов) (5-10 тысяч элементов). Написать краулер, который соберет информацию с этого сайта, которую будем использовать для построения поискового индекса. Желательно, чтобы о товара было несколько полей.

Задание 3. Создать индекс с помощью Lucene -- записать все документы в индекс и что"=нибудь поискать.

Задание 4. Посмотреть, каким образом можно добавить и удалить документ из люценовского индекса и сразу попробовать найти этот документ, если сразу не будет искаться -- надо будет разобраться, в какой момент документ станет доступным (либо сделать коммит, либо еще что"=то).

Задание 5. Добавить поиск по числовым полям.

Задание 6. Посмотреть, как автомат работает в Lucene. Нужно засунуть наш словарь в этот автомат.

Задание 7. Сделать assesment (для документ + запрос сделать оценки, от 0 до 2) и оценить качество поисковой системы. Использовать $NDCG$.

Задание 8. Построить вероятностную модель по нашему индексу. Однограммную (вообще без условной вероятности), двуграммную и трехграммную модель. Попробовать для каждой из этих моделей продолжить фразу.

Задание 9. Обучить нейросеть Word2Vec, Glove или эмебдинг на нашем корпусе товаров и посмотреть похожие ли слова.

Задание 10. Определить, какие есть синонимы в нашем домене. Добавить поиск по синонимам. Классификатор можно не делать, реализовать tf-idf, pmi, близость в векторном пространстве.

Задание 11. Улучшить поиск с помощью машинного обучения.

\section{Булев поиск, обратный индекс, разбор текста}
\subsection{История}
Способы передачи знания:
\begin{enumerate}
\item Устные предания.
\item Письменность.
\item Библиотеки.
\item Интернет.
\end{enumerate}

История письменности:
\begin{enumerate}
\item Глиняные таблички (5000-3000 до н.э.) - в основном бухгалтерский учет.
\item Алфавит (2000-1500 до н.э.).
\item Печатный станок (в 15 веке).
\end{enumerate}

Традиционный поиск:
\begin{itemize}
\item Объем 1mb-10gb.
\item Поиск перебором.
\item Время отклика -- несколько секунд.
\item Один пользователь.
\end{itemize}

Современный поиск:
\begin{itemize}
\item Объем -- весь интернет.
\item Сложные алгоритмы.
\item Время отклика -- 10-500 мс.
\item Миллионы пользователей.
\end{itemize}

Развитие поисковых систем
\begin{itemize}
\item 29 окт 1969г - рождение интернета (передача данных между двумя компьютерами на разных концах Америки)
\item 6 авг 1991г - первый сайт info.cern.ch….., т.к. некоторые данные нужны были сразу группе лиц
\item июнь 1993г - страница “What’s new” о новых сайтах и краткое описание, поддерживалась вручную
\item 1993г - первая поисковая система AliWeb (владельцы сайтов на спец страницах хранили список документов с указанием о том, какие именно данные можно найти в тех или иных документах; поисковая система использовала эти данные, каталогизировала и использовала для запросов пользователей; 2 минуса: нужен список сайтов, на которых искать спец страницы + сами данные вручную определялись владельцами)
\item 1994г - keyword-based поиск (“найди мне то, что я сказал”)
\item 1998г - ранжирование PageRank (“найди мне то, что я имел в виду”) - учитывает авторитетность документа
\end{itemize}

\subsection{Базовые понятия (учить не надо, но полезно)}

\textit{Информационный поиск} - процесс поиска ответа среди неструктурированной документальной информации, призванного удовлетворить информационную потребность пользователя

\textit{Информация} - сведения независимо от формы их представления (в нашем случае: документы - носители информации)

\textit{Запрос} - сформулированная пользователем поисковая потребность

\textit{Результаты поиска} - ответ поисковой системы на запрос пользователя

\textit{Документ} - объект, содержащий в себе информацию (из него извлекаются данные, по кот осущ поиск, он может быть ответом на поисковый запрос пользователя)

\textit{Корпус} - набор документов, кот исп для построения поисковой системы

\textit{Релевантность} - способность конкретного документа успешно удовлетворить инф потребность пользователя, выраж в форме конкретного запроса. Этот термин используется только в контексте пары документ-запрос.

\textit{Полнотекстовый поиск} - это поиск, который в ходе построения ответа пользователю основывается в первую очередь на том, какой текст хранится в самом документе.

\textit{Основные характеристика качества поиска}:
\begin{itemize}
\item Точность - сколько документов из числа найденных поиск системой действительно являются релевантными запросу пользователя
\item Полнота - сколько документов из всех, что релевантны запросу пользователя, смогла найти поиск система
\end{itemize}

\textit{Классификация поисковых систем (учить не надо)} 
\begin{itemize}
\item по хранению данных для поиска (автономная / внешняя (веб-поиск))
\item по типу данных для поиска (текст / нетекстовые данные (видео, аудио, картинка) / метаданные (рег номера, дата создания))
\item по типу корпуса (корпус тематич документов / корпус “случ” документов)
\item по виду запросов (текстовый ввод / голосовой ввод / по образцу (картинке) / альтерн)
\item по результату (набор документов / все релевантные документы / информация (место на карте) / результат вычислений (построенный маршрут)))
\end{itemize}

\subsection{Булев поиск}
Булев поиск -- есть 3 оператора (булева алгебра):
\begin{itemize}
\item AND - A AND B - документ содержит и А, и В
\item OR
\item NOT -- используется в чистом виде редко, в основном AND NOT.
\end{itemize}

Сложный запрос: “лук” AND (“стрельба” OR “стрельбище”) AND NOT “растение”
Наивный подход:
\begin{itemize}
\item найти все док где “лук”
\item найти все док где стрельба
\item где стрельбище
\item операция OR
\item операция AND и т.д… (поэтапно)
\end{itemize}

Очевидно, что каждый раз использовать весь корпус документов для обработки одного запроса пользователя долго и накладно.

\subsection{Матрица инцидентности}
Матрица термин"=документ: столбцы -- названия документов (книг), строки -- термины, единица на пересечении, если это слово присутствует в документе (рис.~\ref{inc_matrix}).

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=1]{resourses/pictures/inc_matrix.jpg}
	\caption{Матрица инцидентности} \label{inc_matrix}
\end{figure}


Если хотим найти документ, в котором встречаются несколько слов -- берем AND всех этих строк, если те, в которых есть хоть какое"=то из слов -- OR и т.д (рис.~\ref{ops_on_vectors}).

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=1]{resourses/pictures/ops_on_vectors.jpg}
	\caption{Булев поиск как операция над векторами} \label{ops_on_vectors}
\end{figure}

Матрица инцидентности в общем случае сильно разрежена => можем хранить только значимые части, т.е. обратный индекс

\subsection{Обратный индекс}
Обратный индекс  (рис.~\ref{reversed_index_0}):
\begin{itemize}
\item Для каждого термина собираем документы, где это слово встречается, заменяем документы на их ID.
\item Получаем для каждого термина отсортированный список документов, где это слово встречается. Размер таких списков будет расти линейно и занимать не больше места, чем исходные документы.
\end{itemize}

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=1]{resourses/pictures/reversed_index_0.jpg}
	\caption{Обратный индекс} \label{reversed_index_0}
\end{figure}


Что ещё можем хранить:
\begin{itemize}
\item позиция слова в документе
\item частота слова в корпусе
\item популярность документа
\end{itemize}


\subsection{Разбор документа}
Чтобы построить индекс, надо сначала его обработать \ref{make_reversed_index}.
\begin{itemize}
\item Выделение терминов.
\item Нормализация.
\end{itemize}

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=1]{resourses/pictures/make_reversed_index.jpg}
	\caption{Разбор документа} \label{make_reversed_index}
\end{figure}

Токен - экземпляр последовательности символов в документе, объединенных в семантическую единицу для обработки

Терм - “нормализованный” токен (регистр, морфология, исправление ошибок)

Нормализация зависит от языка документа: 
\begin{itemize}
\item mit - слово на немецком,
\item MIT - университет. Такие случаи надо обрабатывать по-разному
\end{itemize}

Регистр
\begin{itemize}
\item toLower(all)
\item Возможны исключения (MIT и mit)
\item часто лучше понижать все, т.к. пользователя не заботятся о капитализации в запросах
\end{itemize}

\textit{Точки, запятые, тире и другие знаки}.  В некоторых случаях исключать из индексов, но возникают проблемы.

\textit{Стоп"=слова} есть почти во всех документах коллекции, например, the, be, and, of, a. Их исключают из индексов.

Что делать, если эти слова несут смысловую нагрузку? Используются \textit{биграммы} -- слова разбиваются не по одному, а по парам, тогда стоп"=слово не встречается во всех словах. Но используется это довольно редко.

Stanford University - булев поиск по одному слову может дать не те результаты, поэтому нужно использовать биграммы
Нужно помнить, что при изменении длины граммы, индекс увеличивается

Positional index: для каждого термина из лексикона хранятся словопозиции: 
слово: docID: сколько раз: <pos1, pos2, ...>.  Для поиска фразы можно учитывать расстояние (=1). Исп такой индекс, можно расширить возможности запросов, указывая макс расстояние между словами
существенно увеличивает объем

Для лучшего результата использовать смешанный подход:
Britney Spears - встречается редко, исп positional index;
The Who <- встречается часто, исп биграмму.


\textit{Проблемы токенизации в примерах}:
\begin{itemize}
\item Hewlett-Packard - имена собственные
\item State-of-the-art - устойчивое выражение
\item Co-education - слово с тире
\item San Francisco - нельзя разбивать
\item York University vs. New York University 
\item обработка чисел, дат, телефонных номеров, ip address - разные форматы
\item в китайском нет пробелов, в немецком компаунды и т.д.
\item .NET, O’Reilly - учитывать знаки
\item стоп-слова - слова, кот есть почти во всех документах (the, be, and, of, a)
\item Named Entity Recognition - извлечение объектов, кот означают одно понятие: ФИО, адреса, телефоны, даты, названия песен, фильмов и т.д.
\end{itemize}

\textit{Стемминг} (нормализация) -- например, creates, created, creating преобразовать в creat. Стемминг просто находит у слова окончания и выкидывает его. Для поиска окончаний есть ряд правил, работает хорошо для английского языка. В русском работает очень плохо, поэтому используются другие алгоритмы (Стеммер Портера, Stemka, Mystem).

Ошибки стемминга: run, men, children -- на этих словах стемминг не работает, в таких случаях лучше использовать морфологию.

\textit{Лемматизация} - привести все разные формы одного слова к начальной (каноничной). Заключается в поиске начальной формы (леммы в словаре).Обычно используется конечный автомат.
Что делать со словами, которых нет в словаре? Ищем похожие!

Слово = машинная основа + парадигма (окончание)

Проблемы: омонимия: белки (белка /белок); словоформа (run, men, children - конкретная морфологическая разновидность слова)

\textit{Морфология}:
\begin{itemize}
\item Строится по словарю.
\item Для незнакомых слов используется эвристика, позволяющая находить нормализированную форму слова.
\end{itemize}

Процесс нормализации, реализованный в грамматическом словаре, позволяет убрать из исходного текста грамматическую информацию (падежи, числа, глагольные виды и времена, залоги причастий, род и так далее), оставляя смысловую составляющую.

Два других алгоритма - стемминг и лемматизация - пытаются достичь такого же эффекта, но глубина преобразования текста в них меньше. Другая сторона медали - более существенные затраты вычислительных ресурсов на выполнение всех стадий алгоритма глубокой нормализации.


В качестве примера можно взять предложение
\textit{вижу три села}.
При неблагоприятном стечении обстоятельств нормализация даст на выходе малорелевантный результат:
\textit{видеть тереть сесть}
вместо корректного
\textit{видеть три село}.


\subsection{Быстрое пересечение списков}
Быстрое пересечение списков \ref{quick_intersec}:
\begin{itemize}
\item Сложность n+m.
\item Если пересечение маленькое, то можно сделать быстрее.
\item Можно идти не просто двумя указателями, но при несовпадении каждый раз увеличивать шаг в 2 раза.
\item Также можно разбивать на $\sqrt{n}$ частей и искать в них.
\end{itemize}

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=1]{resourses/pictures/quick_intersec.jpg}
	\caption{Обратный индекс} \label{quick_intersec}
\end{figure}

\subsection{Исправление ошибок}
Кандидаты на исправление:
\begin{itemize}
\item словарь известных слов
\item слово из словаря
\item ближайшее по какой-нибудь метрике
\end{itemize}

В качестве метрики будем использовать расстояние Левенштейна -- минимальное количество символов, которые нужно поменять, добавить, удалить, чтобы из одного слова получить другое, решается с помощью ДП со сложность $O(n\cdot m)$

Первый вариант: перебрать все слова в словаре, для каждой пары подсчитать расстояние. Работает долго. Нужно сузить количество кандидатов.

Второй вариант: k-грамм индекс -- каждое слово разбиваем на k-граммы (по k символов, например опечатка на опе, печ, еча, чат, тка). Для каждой k-граммы список слов, где она встречается. Например, опе -> \dots, операция, опечатка, \dots

Нахождение кандидатов -- объединить списки k-грамм, можно брать только те слова, у которых большой процент совпадения k-грамм. Выбираем топ-100 слов с наибольшим весом.

\section{Сбор данных с web}
Устройство web:
\begin{itemize}
\item HTTP протокол.
\item Ссылки.
\end{itemize}

На основе ссылок строится web"=граф.

Структура URL: <протокол>://<логин>:<пароль>@<хост>:<порт>/<URL-путь>?<параметры>\#<якорь>

Протоколы:
\begin{itemize}
\item http(s)
\item ftp
\item mailto
\item irc
\end{itemize}

<логин>:<пароль>
\begin{itemize}
\item user
\item user:password
\end{itemize}

<хост>:<порт>
\begin{itemize}
\item localhost:8080
\item sgu.ru
\item 192.168.1.1
\end{itemize}

<URL-путь>
\begin{itemize}
\item somedir/somefile.html
\end{itemize}

case-(in)sensetive -- маленькие и большие в названиях файлах равнозначны -- нельзя создать два файла, один с большой буквы, а другой с таким"=же названием, но с маленькой.

<параметры>\#<якорь>
\begin{itemize}
\item someanchor -- в URL не передается
\end{itemize}

Протокол http -- сервер последовательно получает:
\begin{itemize}
\item Стартовая строка: 
	\begin{itemize}
	\item GET index HTTP HTTTP/1.0
	\item Host: sgu.ru
	\end{itemize}
\item Заголовки:
	\begin{itemize}
	\item Responce headers
	\item Request headers
	\end{itemize}
	
	Content type:
		\begin{itemize}
		\item html/text
		\item application/json
		\item application/octet-stream
		\item image/gif
		\end{itemize}
		
		HTTP status code:
		\begin{itemize}
		\item 200 OK
		\item 201 Created
		\item 202 Accepted
		\item 204 No content
		\item 301 Moved Permanently (Location header)
		\item 302 Found, 302 Moved Temporarily
		\item 304 Not modified -- с последнего обращения ничего не изменилось
		\item 400 Bad Request
		\item 401 Unauthorized
		\item 403 Forbidden -- уже авторизовались, но прав не хватает
		\item 404 Not Found
		\item 500 Internal Server Error
		\item 501 Not Implementrd -- функциональность еще не реализована
		\item 503 Service Unavailable
		\item 504 Gateway Timeout
		\end{itemize}
\item Тело сообщения:

HTML -- логическое описание документа.

CSS -- описание внешнего вида документа.

HTML A tag -- тэг со ссылкой.

Селектор -- описание, каким образом мы выберем элементы.

Виды селекторов (первые 4 одинарные, дальше -- каскадные) (код в презентации):
\begin{itemize}
\item div -- <div></div>
\item .note -- выбор по классу
\item \#par -- выбор по уникальному id
\item a[href="test"] 
\item div\#par -- выбрать div, а в нем найти по id 
\item p.note -- выбрать все элементы p с классом note
\item p.note>b -- внутри еще выбрать тег b
\end{itemize}
\end{itemize}

Разбор страницы (библиотеки):
\begin{itemize}
\item jQuery (JS)
\item JSOUP
\item 
\end{itemize}

Инструментарий.

Поисковый робот:
\begin{itemize}
\item Устойчивость -- ошибки сайта, неправильные статусы не ломают работу робота.
\item Вежливость -- робот не делает много запросов, чтобы не сломать сайт (надо ориентироваться на популярность сайта).
\end{itemize}

Схема работы поискового робота: Очередь URL $\rightarrow$ Скачать страницу $\rightarrow$ Анализ $\rightarrow$ Разобрать содержимое $\rightarrow$ Извлечь URL $\rightarrow$ Проверить уникальность URL (или если прошло много времени и надо скачать снова) $\rightarrow$ Очередь URL.

\section{Обзор Lucene}
Первый релиз в марте 2000.

Основные концепции:
\begin{itemize}
\item Document -- то, что индексируется и будет возвращаться в поиске.
\item Field -- поле, у документа может быть много полей с одинаковыми названиями.
\item Term -- разбиваем поле на термы.
\end{itemize}

Field Type:
\begin{itemize}
\item Indexed -- по этому полю будет осуществлен поиск.
\item Stored -- поле будет хранить в базе, его можно будет достать.
\item Tokenized -- поле будет разбиваться на токены и храниться в базе.
\end{itemize}

Типы полей:
\begin{itemize}
\item Text -- токенизируется.
\item String -- не токенизируется.
\item Double -- поддерживается поиск по числовым полям.
\item Float
\item Long
\item Integer
\end{itemize}

Код создания документа в презентации.

commit -- запись данных на диск (не в ОП). В некоторых случаях до такой записи нельзя искать в документе.

Long merge tree -- используется для организации данных.

Стадии добавления документа:
\begin{itemize}
\item Tokenizer -- разбивает текст на токены.
\item Token Filter (возможно несколько) -- выполняет преобразования над токенами (приводит все к нижнему регистру, разбивает на k-граммы, приводит к стандартному виду и т.д.)
\item Index
\end{itemize}

Tokenizerы:
\begin{itemize}
\item StandardTokenizer -- умно все разбивает, находит точки, тире, определяет концы предложения, но при этом сохраняет фразы вроде .NET
\item KeywordTokenizer -- ничего не разбивает, но ищет ключевые слова
\item PatternTokenizer -- настраивается по своим паттернам.
\end{itemize}

TokenFilters:
\begin{itemize}
\item LowerCaseFilter -- приводит к нижнему регистру.
\item StopFilter -- позволяет останавливать поиск после нахождения стоп"=слова.
\item ..
\end{itemize}

Запросы:
\begin{itemize}
\item Query -- класс, от которого унаследованы все остальные запросы.
\item TermQuery -- поиск по терму, найти все документы, у которых есть данный терм. При создании нужно указать имя поля и значение этого поля.
\item BooleanQuery -- позволяет соединять несколько TermQuery с помощью булевого поиска, конструкции:
	\begin{itemize}
	\item SHOULD -- должен присутствовать поиск, но не обязателен.
	\item MUST
	\item MUST\_NOT
	\end{itemize}
\item PhraseQuery -- позволяет учитывать количество слов между найденными словами.
\item TermRangeQuery -- найти все слова, начинающие с определенной последовательности букв (от ab до ac например).
\item PrefixQuery
\item WildcardQuery -- поиск со звездочкой в слове
\item RegExpQuery -- поиск по регулярному выражению, но нельзя, чтобы звездочка стояла в начале слова.
\end{itemize}

Есть queryParser, позволяющий распарсить строку на языке запросов. Но лучше писать свой трансформатор под определенную структуру.

Язык запросов:
\begin{itemize}
\item field:term -- поле и терм
\item field:<<phrase query>> -- если хотим искать по фразе -- нужны кавычки
\item <<phrase query>> -- искать везде фразу, если указать без кавычек -- превратится в or query?
\item Term modifers: te?t, test*, te*t, но нельзя *est, ?est
\item Fuzzy Search esroam~1 -- все термы, для которых расстояние Левенштейна <= 1.
\item Range Searches mod\_date:[20020101 TO 20030101] -- поиск в диапазоне.
\item NOT OR AND
\item + -- must, - -- not, по умолчанию should
\end{itemize}

Задание 3. Создать индекс с помощью Lucene -- записать все документы в индекс и что"=нибудь поискать.

\section{Построение поискового индекса}
Построение индекса:
\begin{itemize}
\item Построение в памяти -- как в задании 1. Проблемы: хранится в ОП, ее может не хватать.
\item Блочное индексирование -- если коллекция не помещается в память, разбиваем на блоки, сохраняя их на диске. При объединении сначала сортируем блоки, а затем их сливаем.
\item Распределенное индексирование -- индекс не помещается на диске одной машины. Кроме того процесс построение индекса может занимать много времени. Исходные документы разбиваются на пачки, каждая машина заведует своей пачкой. Можно индексировать, например, на 50 машинах, а поддерживать индекс всего на 3 самых мощных.
\item Map reduce -- строит единый индекс, который должен помещаться на одну машину. Используется редко. Это парадигма параллельных вычислений:
	\begin{itemize}
	\item split -- разделяет документы по пачкам, каждому документу присваивается id.
	\item map -- разделяем документы на токены, применяем токенайзер, аналайзер.
	\item shuffle -- проводим сортировку информацию по словам, затем по индексу.
	\item reduce -- объединяем списки, получаем готовый обратный индекс.
	\end{itemize}
\item Динамическое индексирование -- в памяти индекс строится динамически, когда он достигает определенного объема -- он записывается на диск.

Для обеспечения надежности одновременно идет запись в индекс памяти и во write ahead log на диске.

Чтобы не бегать по большому количеству блоков, их нужно время от времени сливать. Это можно делать в фоновом процессе, одним из ядер, пока остальные занимаются построением индекса.

Некоторые документы могут оказаться сразу в нескольких блоках, но в некоторых из них они будут помечены как удаленные. Действительно удаление произойдет только при слияние блоков.
\end{itemize}

Организация поиска:
\begin{itemize}
\item Поисковый кеш:
	\begin{itemize}
	\item Кеш документов.
	\item Кеш запросов.
	\item Кеш полей -- сохраняем часто употребляемые слова.
	\end{itemize}
\item Индекс в памяти.
\item Блоки на диске.
\item WAL (write ahead log).
\end{itemize}

Коммит -- заливает индекс из памяти на диск, а оптимайз сливает блоки в памяти.

Задание 4. Посмотреть, каким образом можно добавить и удалить документ из люценовского индекса и сразу попробовать найти этот документ, если сразу не будет искаться -- надо будет разобраться, в какой момент документ станет доступным (либо сделать коммит, либо еще что"=то).

\section{Лекция 5}
\subsection{Поиск по * и перестановочный индекс}
Дописываем в конце слова какой"=нибудь специальный символ, например знак доллара. Кроме того размножаем каждое слово.

test:
\begin{itemize}
\item test\$
\item est\$t
\item st\$te
\item t\$tes
\item \$test
\end{itemize}

Для поиска n*m преобразуем: $\rightarrow$ n*m\$ $\rightarrow$ m\$n*

\subsection{Поиск по числовым полям}
Нужна поддержка поиска по числам в диапазонах.

Для индексации числового значения вместо числа сложим в индекс набор чисел, всякий раз увеличивая количество знаков после запятой. Например для $\pi$: 3, 3.1, 3.14, 3.141 и т.д.

Затем для поиска в интервале выбираем подходящее количество знаков после запятой и берем все такие числа в интервале. Например для поиска от 2 до 3.5 возьмем поиск по 2 и 3, для от 3.1 до 3.25 -- 3.1 и 3.2, для от 3.1 до 3.15 -- 3.1. После этого лишние отфильтровываем.

Кроме того целые числа тоже можно делить по разрядам и класть иногда только высшие разряды, чтобы также выбирать разряд для поиска.

\subsection{Геопоиск}
Бьем карту на кусочки для разных масштабов. Для кусочков используются широта и долгота, но нужно учитывать, что они кодируют разные расстояния в разных точках мира (на экваторе больше, у полюсов меньше).

Так же можно использовать KD"=Trees.

Задание 5. Добавить поиск по числовым полям.

\subsection{Сжатие индекса}
Преимущества сжатия:
\begin{itemize}
\item Меньше требуется ресурсов на хранение.
\item Быстрая загрузка данных с диска в память.
\item Быстрая загрузка данных из памяти в кеш процессора.
\end{itemize}

Закон Хипса: $M \approx kT^b$, где
\begin{itemize}
\item $M$ -- количество различных терминов
\item $T$ -- количество лексем в коллекции
\item $b \approx 0.5$
\item $30 \leqslant k \leqslant 100$
\end{itemize}

Закон Ципфа: $fn \approx \frac{1}{n}$, где $fn$ -- частота $n$-ого слова.

Если не сжимать для каждого слова нужно хранить 8"=байтовый указатель, 2"=байтовое число для количества символов и сами символы.

Для экономии места все строки объединяем в одну и храним только числовые позиции начала каждой из строк.

Можно сжать еще сильнее, храня блоками, храня начало блока, а в блоках размеры слов.

Кроме того, можно в начало блока записать общий префикс и дальше хранить только суффиксы. 

В Lucene Для хранения используют конечный автомат: $a \rightarrow u \rightarrow t \rightarrow o \rightarrow End, a \rightarrow p \rightarrow t$

Задание 6. Посмотреть, как автомат работает в Lucene. Нужно засунуть наш словарь в этот автомат.

\subsection{Сжатие инвертированного индекса}
В индексе есть слово и id"=шники документов, в которых оно встречается. Идшники могут быть большими, поэтому можно хранить не сами идшники, а разницы между ними.

Для этого используется кодирование переменной длины: в каждом байте храним 7 бит числа и один бит на флаг, закончилось ли число. На число может хватит одного или двух битов.

\section{Ранжирование}
\subsection{Зональное ранжирование}
Пример: 

Книги:
\begin{itemize}
\item Автор
\item Заголовок
\item Аннотация
\item Содержание
\end{itemize}

$\sum_{i}^{n}\alpha_if_i$, где $\alpha_i$ -- коэффициент зоны, $f_i$ -- значение зонной функции.

$\sum_{j}^{k}\left(\sum_{i}^{n}\alpha_if_{ij} - r_j\right)^2 \rightarrow \min$

\begin{itemize}
\item Метод наименьших квадратов
\item Квадратичная форма
\item Градиентный спуск
\end{itemize}

Зональное ранжирование в Lucene:
\begin{itemize}
\item Document level boosting -- выставляем веса документам
\item Field level boosting -- выставляем веса полям
\item Query level boosting -- в запросе задаем, у какого слова будет больший вес
\end{itemize}

\subsection{Частота термина и взвешивание}
\begin{itemize}
\item Bag of words
\item Неважен порядок слов
\item Пары документы и запрос
\item Каждому слову в такой паре задается вес
\end{itemize}

\begin{itemize}
\item Основная характеристика -- частота термина в документе.
\item TF -- сколько раз слово встретилось во всех документах.
\item DF -- сколько раз слово встретилось в конкретном документе.
\item Не все слова одинаково полезны.
\item Чтобы избежать этого словам присваивается частота
\end{itemize}

Обратная частота (idf): чем в большем числе документов встретился терм, тем менее он ценен:

$idf = \log\frac{N}{df_t}$

$tf \cdot idf_{t, d} = tf_{t, d} \cdot \log\frac{N}{df_t}$

Векторная модель ранжирования:
\begin{itemize}
\item Документ и слова представляем как вектора
\item Косинусная мера похожести:

$\cos \alpha = \frac{(d_i, d_j)}{||d_i||\cdot||d_j||}$ (в числителе скалярное произведение, потому что не нужно учитывать лишние слова?)

$0 \leqslant \text{sim} \leqslant 1$
\end{itemize}

Сублинейное масштабирование TF:

\begin{equation*}
\begin{cases}
1 + \log tf_{t, d}, & \text{if}\, tf_{t, d} > 0 \\
0, & \text{if}\, tf_{t, d} = 0
\end{cases} 
\end{equation*}

Нормировка tf на максимальный tf:

$a + (1 - a) \frac{tf_{t, d}}{\max tf_{t, d}}$

Lucene ranking:

\begin{equation*}
\text{coord} \cdot \text{queryNorm} \cdot \sum_{t\in q}\sqrt{tf_t}\cdot ifd_t \cdot \text{boost}_t \cdot \text{norm}(t, d)
\end{equation*}

\begin{equation*}
%\text{norm}(t, d) = \text{lengthNorm} \cdot \prod_{\text{f.value} = t} \text{boost_f}
\end{equation*}

%\begin{equiation*}
%\text{queryNorm} = \frac{1}{\text{boost}_q\sqrt{\}}
%\end{equiation*}

Формула BM25: одна из лучших форм для ранжирования, показывает очень хорошие результаты, для дальнейшего улучшения нужно применять уже более сложные методы и языковые модели.

\begin{equation*}
\sum_{i = 1}^{n}ifd(q_i)\frac{tf(k_1+1)}{tf+k_1(1 - b + b\frac{|D|}{\text{avgdl}})}
\end{equation*}

\section{Оценка качества поиска}
Необходимы элементы:
\begin{itemize}
\item Набор документов
\item Набор запросов
\item Набор оценок релевантности относительно каждой пары документ"=запрос
\end{itemize}

Оценка неранжированных результатов поиска:
\begin{itemize}
\item Точность $P = \frac{tp}{tp + fp}$ -- количество релевантных документов, что мы выдали на то, сколько всего мы выдали
\item Полнота $R = \frac{tp}{tp + fn}$ -- сколько релевантных документов мы выдали пользователю на то, сколько всего есть релевантных документов
\item F"=мера $F = \frac{2PR}{P + R}$ -- среднее гармоническое для этих величин
\end{itemize}

В этой метрике не учитывается порядок, поэтому можно использовать метрику MAP (minimum average percision):
\begin{equation*}
MAP(Q) = \frac{1}{|Q|}\sum_{j = 1}^{|Q|}\frac{1}{m_j}\sum_{k = 1}^{m_j}P(R_{jk})
\end{equation*}

$R_{jk}$ -- результаты запроса с j по k

Метрика NDCG ():

\begin{equation*}
NDCG(Q, k)= \frac{1}{|Q|}\sum_{j = 1}^{|Q|}Z_k\sum_{m = 1}^{k}\frac{2^{R(j, m)} - 1}{(1 + m)}
\end{equation*}

$R(j, m)$ -- релевантность m документа в j запросе

$Z_k$ -- подбираются таким образом, чтобы $NDCG$ была равна 1 при идеальной 

Задание 7. Сделать assesment (для документ + запрос сделать оценки, от 0 до 2) и оценить качество поисковой системы. Использовать $NDCG$.

\section{Вероятностная модель}
Вероятностная модель ранжирования -- считаем вероятность того, что документ релевантный.

В формулах d -- document, q -- query, R -- является ли документ релевантным. Используется условная вероятность.

Сортировать можно не по полученным вероятностям, а по их отношениям.

Бинарная модель: учитываем вхождение слова только один раз, слова считаются независимыми.

Если в произведении нет $w_i = 0$ или $w_i = 1$ -- оно не зависит от запроса.

df -- document frequency, tf -- term frequency, S -- общее количество релевантных документов, N -- общее количество документов, s -- количество релевантных документов, в которых встретился данный термин из запроса.

\section{Языковые модели}
Вероятностная модель порождения языка -- есть некоторая фраза, мы можем дописать туда какое"=то слово.

Закрыв последнее слово фразы пытаемся угадать его. Лучше всего взять последние 4 слова и считать исходя из них.

Можем считать условные вероятности для каждого слова при всех предыдущих, можно учитывать условную вероятность только для последнего слова, а можно брать только вероятности сами слов.

$P(t_1, t_2, t_3, t_4) = P(t_1)\cdot P(t_2 | t_1) \cdot P(t_3 | t_2, t_1) \cdot P(t_4 | t_1, t_2, t_3)$ -- наиболее правильная модель, пытаемся ее упростить.

$P_{uni}(t_1, t_2, t_3, t_4) = P(t_1)P(t_2)P(t_3)P(t_4)$ -- вероятностная модель (униграммная).

$P_{bi}(t_1, t_2, t_3, t_4) = P(t_1)P(t_2 | t_1)P(t_3 | t_2)P(t_4 | t_3)$ -- биграмная модель.

Задание 8. Построить вероятностную модель по нашему индексу. Униграммную (вообще без условной вероятности), двуграммную и трехграммную модель. Попробовать для каждой из этих моделей продолжить фразу.

Для языка существует много моделей. Языковая модель для документа, для корпуса.

$P(t | M_c)$ -- языковая модель для корпуса.

$P(t | M_d)$ -- языковая модель для документа.

$P(t | t_1)$

Модель правдоподобия запроса.

$P(d | q) = \frac{P(q |d)P(d)}{P(q)} \approx P(q | d)P(d)$

$P(d) = \frac{L_d!}{tf_1!\dots tf_n!}P(t_1)^{tf_1}\dots P(t_n)^{tf_n}$

$L_d = \sum_{i = 1}^{n}tf_i$

$P(q | d) = P(q | M_d) = \frac{L_q!}{tf_{t_1, q}\dots tf_{t_k, q}}\prod P(t_k | M_d)^{tf_{t_k, q}}$

Данная модель не подходит, если термин из запроса не встречается в документе.

$P(t | M_d) = \lambda P_{MLE}(t | M_d) + (1 - \lambda) P_{MLE}(t | M_c)$

$P(t | M_d) = \frac{tf_{t, d} + \alpha P_{MLE}(t | M_c)}{L_d + \alpha}$

\section{Современные языковые модели}
Проблемы классического подхода: требуется большой объем оперативной памяти чтобы держать модель.

В корпус включаются только те последовательности слов, что встретились хотя бы 5 раз.

Языковая модель как рекуррентная нейронная сеть.

Слово представляется как вектор определенной размерности (от 50 до 300 в зависимости от задачи). Это удобно использовать как вход для нейронных сетей. 

Word2Vec CBOW. По контексту нужно предсказать слово: word t-2, word t-1, word t+1, word t+2 $\rightarrow$ SUM $\rightarrow$ (softmax) word t.

Word2Vec Skip-gramm. По слову нужно предсказать контекст: наоборот из одного слова в 4.

Glove. По двум словам предсказать совместную встречаемость. word 1, word 2 $\rightarrow$ count
\begin{equation*}
J = \sum_{i, j}f(x_{ij})(w_i^T w_j - \log X_{ij})^2
\end{equation*}

Задание 9. Обучить нейросеть Word2Vec, Glove или эмебдинг? на нашем корпусе товаров.

Простая ячейка. Вечная память или беспамятство -- если сеть запомнила информацию, то она ее не забудет и из"=за этого может на запомнить нужную информацию.

LSTM -- long short term memory. Лишена проблемы затухания градиентов, память очищается.

RNN for text. Слово $\rightarrow$ embeding $\rightarrow$ LSTM $\rightarrow$ Out. LSTM $\rightarrow$ LSTM  $\rightarrow \dots$.

Bi-directional RNN for text. LSTM $\leftrightarrow$ LSTM $\leftrightarrow \dots$. Может предсказывать слово в середине по контексту.

Depp netwroks. Слово $\rightarrow$ embeding $\rightarrow$ LSTM $\rightarrow$ LSTM $\rightarrow$ Out.

Stacked depp networks. В Out идут стрелки изо всех уровней LSTM.

Недостаток такой архитектуры: при переводе между языками порядок слов может меняться.

Механизм внимания позволяет избежать этой проблемы. Ищем H2 по выходам O1, O2, O3, O4 с различными весами. Для этого строится блок нейронной сети (linear softmax c 4 выходами (сумма равна 1)). С ее помощью определяем веса w1, w2, w3, w4. H1 -- выход нейронной сети для предыдущего слова. H3, H4 -- будущие выходы для следующих слов.


Что еще можно посмотреть (рассматривать не будем):
\begin{itemize}
\item AWD-LSTM -- ряд трюков, чтобы лучше натренировать модель.
\item Transformer -- другая архитектура, работает сразу с целыми предложениями.
\item ULMFit -- transfer learning для текстов на LSTM.
\item BERT -- transfer learning для текстов на Transformer.
\end{itemize}

\section{Синонимия}
Используется два поля -- исходное поле и поле с синонимами. Полю с синонимами дают более низкий вес.

Синонимы добавляются с помощью специального фильтра.

Подходы для получения синонимов:
\begin{itemize}
\item Использовать уже готовые словари синонимов. Это работает не очень хорошо, поэтому обычно из словаря выкидывают то, что не нужно в данном домене.
\item Автоматически строить словари.
\item Латентно семантическое индексирование -- устаревший способ, не рассматриваем.
\end{itemize}

Автоматическое построение синонимов. Какими свойствами должны обладать синонимы:
\begin{itemize}
\item Должны быть в одном и том же контексте.
\item Они должны находится в одних и тех же документах.
\item Если речь идет про веб, то в тексте ссылки может быть синоним на заголовок документа.
\end{itemize}

Используем контекст слова, обычно длина окна в 3-10 слов. Можно использовать Google ngrams.

Если просто брать топ-1000 -- будет много просто популярных слов. Чтобы избежать этого используем меры схожести:
\begin{itemize}
\item tf-idf
\item point wise mutual information -- $PMI(word_1, word_2) = log_2\frac{P(word_1, word_2)}{P(word_1)P(word_2)}$ Если слова независимые, $PMI = 0$
\end{itemize}

Другие способы получения синонимов:
\begin{itemize}
\item Транслитерация

Вероятностная модель транслитерации:

$P(word) = P(l_1)P(l_2 | l_1)\dots P(l_n | l_1 \dots l_{n - 1})$

\item Словообразовательные расширения: Саратов -- Саратовский
\item Аббревиатуры
\item Близость в пространстве вложений
\end{itemize}

\section{Машинное обучение в ранжировании}
Признаки для ранжирования на примере Google:
\begin{itemize}
\item Факторы домена:
	\begin{itemize}
	\item Возраст домена -- чем старше, тем луче
	\item История домена -- какие были сайты на этом домене
	\item Привязка к стране
	\item Публичный или приватный whois
	\item Ключевое слово в домене
	\item Ключевое слово в поддомене
	\item Оштрафован ли владелец whois
	\item Длительность регистрации домена
	\end{itemize}
	
\item Факторы оптимизации сайта
	\begin{itemize}
	\item Ключевые слова в h1, h2, h3, h4, url, title, meta
	\item Размер страниц
	\item Уникальность наполнения
	\item Исходящие ссылки
	\item Видео и фото на сайте
	\item Скорость загрузки
	\item Битые ссылки
	\item Правильность html
	\item Частота обновления
	\end{itemize}
	
\item Факторы уровня сайта
	\begin{itemize}
	\item Уникальность сайта
	\item Обновления сайта
	\item Наличие карты сайта
	\item Место нахождение сервера
	\item Наличие обратной связи
	\item Количество страниц
	\item Юзабилити сайта
	\item Доступность сайта -- как часто сайт находится не техобсулживании или подобном
	\item Удобство навигации
	\end{itemize}
	
\item Обратные ссылки
	\begin{itemize}
	\item Значение PageRank
	\item Возраст ссылки
	\item Количество ссылающихся страниц
	\item Ссылки с доменов .edu .gov
	\item Ссылки, добавленные пользователями
	\item Качество ссылающего домена
	\item Разнообразие типов ссылок
	\item Позиция ссылки на странице
	\item Слова в тексте ссылки и рядом стоящие слова
	\end{itemize}
	
\item Взаимодействие пользователей
	\begin{itemize}
	\item Количество переходов по данному запросу
	\item Количество переходов по всем запросам
	\item Прямой трафик -- переход из закладок или набирая адрес в ручную
	\item Количество возвратов -- как часто возвращается в поисковик после просмотра этого сайта
	\item Почтовый трафик -- как часто переходят с почтовых сервисов
	\item Количество комментариев 
	\item Время проведенной на сайте
	\end{itemize}
	
	Для большинства этих измерений нужно на сайте ставить специальные метрики, например Яндекс.Метрика или Гугл аналитику.
\item Специальные правила

	\begin{itemize}
	\item Запрос заслуживает свежести -- для новостей нужны ответы последних пары дней
	\item Запросы заслуживают разнообразия -- например для просто запроса о холодильнике нужно выдать ему и магазины, и обзоры.
	\item История запросов пользователя
	\item Учет географии
	\item Доменное разнообразие
	\item Авторское право -- ссылки, скрытые по запросу правообладателя
	\item Подмешивание других вертикалей -- картинки, новости
	\end{itemize}
	
\item Социальный сигналы

	\begin{itemize}
	\item Количество твитов
	\item Количество лайков на facebook, g+, vk
	\item Авторитетность пользователя
	\item Количество репостов, ретвитов
	\item Релевантность аккаунтов
	\end{itemize}
	
\item Факторы спама

	\begin{itemize}
	\item Фермы контента -- сайты с кучей контента, созданные исключительно для денег с рекламы
	\item Ссылки на плохие сайты
	\item Всплывающая реклама
	\item Реклама перед содержанием
	\item Сгенерированный контент
	\item Скрытые редиректы
	\end{itemize}
	
\item Факторы спама вне домена

	\begin{itemize}
	\item Быстрый приток ссылок
	\item Ссылки с одного и того же ip
	\item Покупка и продажа ссылок
	\item Ручной штраф
	\item Релевантность ссылающего домена
	\end{itemize}
\end{itemize}

Ранжирование для товаров:
\begin{itemize}
\item Сколько раз покупался товар
\item Количество отзывов
\item Количество положительных отзывов
\item Разнообразие категории товаров
\end{itemize}

Learning to rank:
\begin{itemize}
\item Поточеченый подход
\item Попарный подход
\item Списочный подход
\end{itemize}

\section{Learning to rank}


\begin{itemize}
\item Поточечный подход -- каждой паре число"=документ нужно присвоить число, по которому будет сортировать.
\item Попарный подход -- тройка: два документа и запрос, для них определяем порядок, какой из документов лучше подходит. Подходит, когда мы не можем однозначно присвоить число.
\item Списочный подход -- получаем список документов и сортируем его. Практически не используется.
\end{itemize}

Нормирование параметров -- перед тем, как заниматься ранжированием, нужно нормировать параметры. У параметров могут быть разные масштабы, поэтому некоторые параметры будут играть значительно большую роль.

$\frac{e^{ax}}{e^{ax} + 1}$

Поточечный подход:
\begin{itemize}
\item Пара документ запрос -- оценка релевантен или нет.
\item Каждой паре соответствует набор признаков. 
\item Задача классификации $(d_i, q_i) = (x_{i1}, x_{i2}, \dots, x_{in}) \rightarrow r_i$
\end{itemize}

Самый простой способ -- линейная регрессия. Разбирали?

SVM классификация -- строим прямую разделяющую, чем дальше от прямой -- тем выше (ниже) ранг. $r_i = (\overline{w}, \overline{x}_i) + b$

Логистическая регрессия: 

$P(r = 1 | x_i) = \frac{e^{\sum \alpha_j x_{ij} + b}}{e^{\sum \alpha_j x_{ij} + b}}$

$P_{actual}(r = 1 | x_i) \log P(r = 1 | x_i) + (1 - P_{actual} (r = 1 | x_i)) \log (1 - P(r = 1 | x_i))$

\subsection{Попарный подход}
\begin{itemize}
\item Исходные данные -- запрос q и пары документов (d1, d2).
\item Для каждой пары документов известно, который подходит лучше.
\item Нужно построить функцию ранжирования f(d).
\end{itemize}

Набор параметров и функция ранжирования: 

$f(d_i) = f(x_1, x_2, \dots, x_n)$

$f(d_i) = \sum_i^n\alpha_i x_i$

Модели:
\begin{itemize}
\item RankNet:
	\begin{itemize}
	\item $P_{ij}$ -- вероятность, что документ $d_i$ стоит выше, чем документ $d_j$
	\item $P_{act ij}$ -- наблюдаемая величина, принимает значения 0, 1
	\item $P_{ij} = \frac{\exp^{o_{ij}}}{\exp^{o_{ij}} + 1}$, $o_{ij} = f(d_i) - f(d_j)$
	
	Нужно восстановить функцию ранжирования из функции потерь
	
	\item $L(D) = \sum_{q_k, d_i, d_j} -P_{act ij}\log P_{ij} - (1 - P_{act ij} \log(1 - P_{ij}))$
	
	\item Здесь очень важно использовать нормировку параметров.
	\end{itemize}
	
\item FRank:
	\begin{itemize}
	\item Функция потерь -- $L(D) = \sum_{q_k, d_i, d_j} \left[1 - (\sqrt{P_{act ij} P_{ij}} + \sqrt{(1 - P_{act ij})(1 - P_{ij})})\right]$
	
	\item Перед использованием алгоритма делаем преклиппинг -- максимальная и минимальная вероятность не 0 и 1, а например $(1e-5, 1 - 1e-5)$.
	\end{itemize}
	
\item Rank SVM:
	\begin{itemize}
	\item В оригинальном алгоритме SVM стараются сделать так, чтобы максимальный запрос был около прямой.
	\item У нас есть прямая, которая разделяет два множества -- релевантные и нерелевантные.
	\item Для каждой точки вводим положительный штраф $\xi_i$, их сумму стараемся минимизировать.
	\item Функция потерь:
	\begin{equation*}
	\begin{cases}
	\|w\| + C \sum \xi_i \rightarrow \min \\
	y_i(w x_i - b) \leqslant 1 - \xi_i \\
	\xi_i \geqslant 0
	\end{cases}
	\end{equation*}
	
	\item Затем решается какими"=то хитрыми методами (не стохастическим градиентом, но можно и им, ибо проще).
	
	\item Функция потерь (вторая? в презентации разные картинки)
		\begin{equation*}
		\begin{cases}
		||w|| + C \sum \xi_{ij} \rightarrow \min \\
		d_{ij}(w, x_i - x_j) \leqslant 1 - \xi_{ij} \\
		\xi_{ij} \geqslant 0
		\end{cases}
		\end{equation*}
	\end{itemize}
	
\item SortNet 
\end{itemize}

Недостатки попарного подхода:
\begin{itemize}
\item Следующие варианты ранжирования не различаются при попарном подходе (p -- perfect, g -- good, b -- bad)
\item g p g b b
\item p g b g b
\item Не учитывают количество документов на уровне запроса -- есть 2 запроса, в одном нашлось много правильных документов, в другом документов вообще мало и все неправильные. Если используем попарный подход напрямую -- получаем неправильную среднюю точность.
\item Нужно дополнительно настраивать попарный подход, чтобы избежать этих проблем.
\end{itemize}

Это лечится с помощью приписывания веса запросу и паре документов -- чем больше пар в запросе, тем должен быть меньше вес у каждой пары и запроса.

Припишем эти веса в нашу функцию потерь (ранжирования?). Проходим эпоху с ними, затем стохастическим градиентом обрабатываем эти веса.

Изменения в RankSVM:





% Раздел "Заключение"
%\conclusion

% Список литературы
%\bibliographystyle{gost780uv}
%\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением
%\appendix

\end{document}
